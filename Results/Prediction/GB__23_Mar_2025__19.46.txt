GB:
Training done. Time taken: 2 days, 20:44:03.708924
params tried:
{'estimator__classifier__max_depth': [10, 12, 15, 17], 'estimator__classifier__max_features': [0.7, 0.85, 0.9, 1], 'estimator__classifier__min_samples_split': [2, 3, 4], 'estimator__classifier__n_estimators': [800], 'estimator__classifier__random_state': [93], 'estimator__oversampler__k_neighbors': [3, 5, 7], 'estimator__oversampler__sampling_strategy': ['auto']}

Best training make_scorer(f1_score, response_method='predict', average=micro, zero_division=0) score: 0.436. Best model params:
{'estimator__classifier__learning_rate': 0.01, 'estimator__classifier__max_depth': 5, 'estimator__classifier__max_features': 1.0, 'estimator__classifier__min_samples_leaf': 3, 'estimator__classifier__n_estimators': 800, 'estimator__classifier__random_state': 93, 'estimator__classifier__subsample': 0.7, 'estimator__oversampler__k_neighbors': 3, 'estimator__oversampler__sampling_strategy': 'auto'}.

Best GB model performance on test data:
              precision    recall  f1-score   support

           0       0.62      0.63      0.63      2117
           1       0.33      0.47      0.39       847
           2       0.16      0.23      0.19       450
           3       0.54      0.58      0.56      1653
           4       0.22      0.48      0.30       501
           5       0.20      0.40      0.27       290
           6       0.29      0.60      0.39       617
           7       0.23      0.39      0.29       458
           8       0.39      0.62      0.48       466
           9       0.23      0.37      0.28       398

   micro avg       0.38      0.53      0.44      7797
   macro avg       0.32      0.48      0.38      7797
weighted avg       0.42      0.53      0.46      7797
 samples avg       0.38      0.54      0.41      7797

Confusion matrices:
 Predicted: 
   N | P  
[[TN, FP] 
[FN, TP]] 
 
[[[1200  806]
  [ 775 1342]]

 [[2466  810]
  [ 448  399]]

 [[3146  527]
  [ 348  102]]

 [[1664  806]
  [ 691  962]]

 [[2744  878]
  [ 259  242]]

 [[3363  470]
  [ 173  117]]

 [[2571  935]
  [ 244  373]]

 [[3073  592]
  [ 281  177]]

 [[3199  458]
  [ 175  291]]

 [[3227  498]
  [ 251  147]]]
