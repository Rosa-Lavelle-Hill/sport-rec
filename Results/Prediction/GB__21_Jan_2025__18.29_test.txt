GB:
Training done. Time taken: 0:00:42.855771
params tried:
{'estimator__classifier__min_samples_split': [2], 'estimator__classifier__max_depth': [5], 'estimator__classifier__n_estimators': [10], 'estimator__classifier__random_state': [93], 'estimator__classifier__max_features': [0.3]}

Best training make_scorer(f1_score, response_method='predict', average=micro, zero_division=0) score: 0.408. Best model params:
{'estimator__classifier__max_depth': 5, 'estimator__classifier__min_samples_leaf': 2, 'estimator__classifier__n_estimators': 10, 'estimator__classifier__random_state': 93}.

Best GB model performance on test data:
              precision    recall  f1-score   support

           0       0.62      0.63      0.63      2117
           1       0.31      0.57      0.40       847
           2       0.13      0.56      0.21       450
           3       0.52      0.63      0.57      1653
           4       0.18      0.72      0.29       501
           5       0.15      0.56      0.23       290
           6       0.26      0.71      0.38       617
           7       0.19      0.54      0.28       458
           8       0.34      0.73      0.46       466
           9       0.19      0.57      0.29       398

   micro avg       0.31      0.63      0.41      7797
   macro avg       0.29      0.62      0.37      7797
weighted avg       0.40      0.63      0.46      7797
 samples avg       0.30      0.62      0.39      7797

Confusion matrices:
 Predicted: 
   N | P  
[[TN, FP] 
[FN, TP]] 
 
[[[1176  830]
  [ 776 1341]]

 [[2221 1055]
  [ 365  482]]

 [[1916 1757]
  [ 196  254]]

 [[1508  962]
  [ 613 1040]]

 [[1969 1653]
  [ 138  363]]

 [[2912  921]
  [ 129  161]]

 [[2234 1272]
  [ 177  440]]

 [[2596 1069]
  [ 212  246]]

 [[2991  666]
  [ 125  341]]

 [[2767  958]
  [ 171  227]]]
